{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab11.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11: Logistic Regression\n",
    "In this lab, you will get practice fitting a logistic regression model on NBA data.\n",
    "\n",
    "### Due Date \n",
    "This assignment is due on **Monday, November 4 at 11:59pm**.\n",
    "\n",
    "### Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about this assignment, we ask that you **write your solutions individually**. If you discuss the assignment with others, please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "sns.set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nba.csv')\n",
    "df[\"WON\"] = df[\"WL\"]\n",
    "df[\"WON\"] = df[\"WON\"].replace(\"W\", 1)\n",
    "df[\"WON\"] = df[\"WON\"].replace(\"L\", 0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: 1D Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture we buit a logistic regression classifier for the NBA data loaded above. Specifically, our model took an observation $x$ and a parameter vector $\\beta$ and used them to generate a prediction $\\hat{y}$. Note that in this question we will assume that $x$ is a one-dimensional scalar.\n",
    "\n",
    "In this case, our predictions represented the probability that the observation belonged to a specific category. In lecture the category was whether or not the team won. That is, $\\hat{y} = P(Y = 1 | x)$, where $Y = 1$ indicates that the team we're observing won the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1a: Implementing a 1D Logistic Model\n",
    "\n",
    "As discussed in lecture, the prediction of our model is $ \\hat{y} = \\sigma(x \\hat{\\beta})$. _Note: Here, both $x$ and $\\hat{\\beta}$ are scalars, not vectors._ \n",
    "\n",
    "In this lab we'll start by trying to build a model that predicts the winning probability as a function of the number of points that a team scored.\n",
    "\n",
    "Below, first define `sigma` to be the sigmoid function we saw in lecture. Then, fill in `predicted_probability_of_winning_given_pts` so that it returns the correct prediction. Your function should work for both scalar and array arguments for `pts`. That is, `predicted_probability_of_winning_given_pts(100, 0.01)` should return a single value (0.731) and `predicted_probability_of_winning_given_pts(np.array([100, 110])), 0.01)` should return an array of values (0.731, 0.750). \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(t):\n",
    "    # BEGIN SOLUTION\n",
    "    return 1 / (1 + np.exp(-t)) \n",
    "    # END SOLUTION\n",
    "\n",
    "def predicted_probability_of_winning_given_pts(pts, beta):\n",
    "    # BEGIN SOLUTION\n",
    "    t = pts * beta \n",
    "    return sigma(t) \n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring an Example Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we pick $\\hat{\\beta}$ = 0.01. We can generate predictions for each of the games in our real world dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.01\n",
    "x = df[\"PTS\"]\n",
    "y_obs = df[\"WON\"]\n",
    "y_hat = predicted_probability_of_winning_given_pts(x, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at our predictions, we see that every team is given a greater than 50 percent prediction of winning based on their number of points. This suggests a problem with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_hat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what's going on, we make a plot of the prediction our model will make as a function of the number of points scored for $\\hat{\\beta} = 0.01$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.01\n",
    "pts = np.linspace(0, 140, 140)\n",
    "plt.plot(pts, predicted_probability_of_winning_given_pts(pts, beta), color = 'orange')\n",
    "plt.ylabel(\"Win Probability\")\n",
    "plt.xlabel(\"Points Scored\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide the actual results from the NBA dataset as blue stars for comparison to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.01\n",
    "pts = np.linspace(0, 140, 140)\n",
    "plt.plot(pts, predicted_probability_of_winning_given_pts(pts, beta), color = 'orange')\n",
    "plt.plot(df[[\"PTS\"]], df[[\"WON\"]], 'b*')\n",
    "plt.ylabel(\"Win Probability\")\n",
    "plt.xlabel(\"Points\")\n",
    "plt.legend(['$\\hat{y}$', 'y']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1b\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "-->\n",
    "\n",
    "Is this model reasonable? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:** No, no matter a team's score, it seems to always have a greater than 50% chance of winning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1c\n",
    "\n",
    "Try playing around with other beta values. You should observe that the models are all pretty bad, no matter what $\\beta$ you pick. Explain why below.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:** No matter what beta we pick, the win probability is always at least 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Adding an Intercept Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe your plot(s) from the previous part of this lab, you'll see that the chance of winning is always at least 0.5 under our model. This is unreasonable, e.g. suppose a team somehow scored only 36 points, they'd have no chance of winning in an NBA game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with this, we should add another feature to our model. Specifically, we'll add a bias term, i.e. a feature that is equal to 1 for all observations. We've done this for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_and_bias = df[[\"PTS\"]].copy()\n",
    "points_and_bias[\"bias\"] = np.ones(len(points_and_bias))\n",
    "points_and_bias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression generalizes to multiple features in exactly the same manner as linear regression.\n",
    "\n",
    "Recall that whereas linear regression on one parameter gave predictions $\\hat{y} = x \\hat{\\beta}$, multiple linear regression gave predictions $\\hat{y} = \\vec{x} \\cdot \\vec{\\hat{\\beta}} = \\vec{x}^T \\vec{\\hat{\\beta}} = \\sum_{i = 1}^p x_i \\hat{\\beta}_i$.\n",
    "\n",
    "Logistic regression generalizes in exactly the same way. That is logistic regression in 1 variable is given by $\\hat{y} = \\sigma(x \\hat{\\beta})$, whereas multiple logistic regression is given by $\\hat{y} = \\sigma(\\vec{x} \\cdot \\vec{\\hat{\\beta}}) = \\sigma(\\vec{x}^T \\vec{\\hat{\\beta}}) = \\sigma(\\sum_{i = 1}^p x_i \\hat{\\beta}_i)$.\n",
    "\n",
    "Fill in the function below so that it returns predictions as described above. As in question 1, your model should be able to handle scalar and array arguments for x. For example `predicted_probability_of_winning_given_features(X.iloc[0:3, :], [0.1, -10])` should return a list (or series) (or numpy array) of the values `[0.6899744811276126, 0.5, 0.21416501695744153]`.\n",
    "\n",
    "Your function only needs to work for array inputs to `x`. That is, your code does not need to work properly for `predicted_probability_of_winning_given_features(110, [0.1, -10])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_probability_of_winning_given_features(X, beta):\n",
    "    # BEGIN SOLUTION\n",
    "    t = X @ beta\n",
    "    return sigma(t)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two parameters $\\beta_1$ and $\\beta_2$. Suppose $\\beta_1 = 0.001$ and $\\beta_2 = 2$. We can compute the predicted probability that each team won during each game as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta = np.array([0.001, 2])\n",
    "predicted_probability_of_winning_given_features(points_and_bias.iloc[0:3, :], beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_and_bias.iloc[0:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a bias term, we have more freedom to adjust our model.\n",
    "\n",
    "For example, if $\\beta_1 = 0.05$ and $\\beta_2 = -5$, we get the curve below. Here, the prediction of your model is $\\sigma(\\beta_1 \\times PTS + \\beta_2)$. That is, $\\beta_1$ is the weight of `PTS`, and $\\beta_2$ is the weight of the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = [0.05, -5]\n",
    "pts = np.linspace(50, 160, 111).reshape(-1, 1)\n",
    "bias = np.ones(len(pts)).reshape(-1, 1)\n",
    "point_range_and_bias = np.hstack((pts, bias))\n",
    "plt.plot(pts, predicted_probability_of_winning_given_features(point_range_and_bias, beta), 'orange')\n",
    "plt.ylabel(\"Win Probability\")\n",
    "plt.xlabel(\"Points\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as before, we can also plot the actual data from our NBA dataset for comparison with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"PTS\"], df[\"WON\"], 'b*')\n",
    "beta = [0.05, -5]\n",
    "pts = np.linspace(50, 160, 111).reshape(-1, 1)\n",
    "bias = np.ones(len(pts)).reshape(-1, 1)\n",
    "point_range_and_bias = np.hstack((pts, bias))\n",
    "plt.plot(pts, predicted_probability_of_winning_given_features(point_range_and_bias, beta), 'orange')\n",
    "plt.ylabel(\"Win Probability\")\n",
    "plt.xlabel(\"Points\")\n",
    "plt.legend(['$\\hat{y}$', 'y']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2b\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "-->\n",
    "Using the plot above, try adjusting $\\beta_2$ (only). Describe how changing $\\beta_2$ affects the prediction curve. Provide your description in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:** Increasing $\\beta_2$ shifts the curve to the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2c\n",
    "Now using the plot below try adjusting $\\beta_1$ and $\\beta_2$ such that you get a sharp curve that is centered at 100 points. In the cell below `beta` should be a list with your chosen values of $\\beta_1$ and $\\beta_2$.\n",
    "\n",
    "- By \"centered at 100 points\", we mean that $\\hat{y}$ should be equal to 0.5 when $x = 100$.\n",
    "- By \"sharp\", we mean that the probability should be less than 5% percent for $x = 80$, and greater than 95% for $x = 100$.\n",
    "- *Hint*: $\\sigma(t) = 0.5$ when $t = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"PTS\"], df[\"WON\"], 'b*')\n",
    "beta = [0.2, -20] # SOLUTION\n",
    "pts = np.linspace(50, 160, 111).reshape(-1, 1)\n",
    "bias = np.ones(len(pts)).reshape(-1, 1)\n",
    "point_range_and_bias = np.hstack((pts, bias))\n",
    "plt.plot(pts, predicted_probability_of_winning_given_features(point_range_and_bias, beta), 'orange')\n",
    "plt.ylabel(\"Win Probability\")\n",
    "plt.xlabel(\"Points\")\n",
    "plt.legend(['$\\hat{y}$', 'y']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Provide your $\\beta_1$ and $\\beta_2$ in the cell below.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = 0.2 # SOLUTION\n",
    "beta2 = -20 # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Optimizing Multiple Logistic Regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now work towards finding the optimal beta $\\vec{\\hat{\\beta}}$ for our given data.\n",
    "\n",
    "_Note: In the previous question, we referred to our $\\beta$s without a hat, since we had yet to find the optimal values of $\\vec{\\beta}$ procedurally._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3a: Calculating MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function `mse_for_model_on_NBA_data(beta)` that takes in a value of $\\vec{{\\beta}}$ and returns the MSE on the dataset from above. You will first need to define the function `mse(y_obs, y_hat)` which finds the mean squared error between `y_obs` and `y_hat`.\n",
    "\n",
    "**Hint:** You need to compute $\\hat{y}$ using the given $\\vec{\\beta}$, then the mean squared error between $\\hat{y}$ and the observed data $y$.\n",
    "\n",
    "**Hint:** Use `points_and_bias` and `df[\"WON\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_obs, y_hat):\n",
    "    return np.mean((y_obs - y_hat)**2) # SOLUTION\n",
    "\n",
    "def mse_for_model_on_NBA_data(beta):\n",
    "    # BEGIN SOLUTION\n",
    "    y_hat = predicted_probability_of_winning_given_features(points_and_bias, beta)\n",
    "    y_obs = df[\"WON\"]\n",
    "    return mse(y_obs, y_hat)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q3a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting MSE\n",
    "\n",
    "The cell below plots your MSE function. We're providing this plot purely for your edification. Warning: This code can be pretty slow and might take a minute or two to run.\n",
    "\n",
    "Note that the surface has a huge almost completely flat region. This means this loss function is very difficult to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "num_points = 50 # increase for better resolution, but it will run more slowly. \n",
    "\n",
    "if (num_points <= 100):\n",
    "\n",
    "    uvalues = np.linspace(-0.3, 0.3, num_points)\n",
    "    vvalues = np.linspace(-20, 20, num_points)\n",
    "    (u,v) = np.meshgrid(uvalues, vvalues)\n",
    "    thetas = np.vstack((u.flatten(),v.flatten()))\n",
    "\n",
    "    MSE = np.array([mse_for_model_on_NBA_data(t) for t in thetas.T])\n",
    "\n",
    "    loss_surface = go.Surface(x=u, y=v, z=np.reshape(MSE, u.shape))\n",
    "\n",
    "    fig = go.Figure(data=[loss_surface])\n",
    "    fig.update_layout(scene = dict(\n",
    "        xaxis_title = \"theta0\",\n",
    "        yaxis_title = \"theta1\",\n",
    "        zaxis_title = \"MSE\"))\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Picking num points > 100 can be really slow. If you really want to try, edit the code above so that this if statement doesn't trigger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3b: Minimizing MSE\n",
    "\n",
    "Using `scipy.optimize.minimize`, find the optimal $\\vec{\\hat{\\beta}}$.  Give your answer as `beta_hat_1` and `beta_hat_2`. The resulting MSE should be less than 0.2.\n",
    "\n",
    "Note: Your starting guess should be (0, 0). If you start somewhere over in the flat region like (0, 20), then scipy.optimize.minimize will get stuck.\n",
    "\n",
    "Note: The test for this question requires that you did Question 3a correctly.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "optimal_beta = minimize(mse_for_model_on_NBA_data, x0 = [0, 0])['x'] # SOLUTION\n",
    "beta_hat_1 = optimal_beta[0] # SOLUTION\n",
    "beta_hat_2 = optimal_beta[1] # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q3b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3c\n",
    "\n",
    "Finally, let's try to understand how our model can be practically useful. As we'll see in lecture on 11/5, we often convert our logistic regression models into a concrete prediction by thresholding. That is, if our $\\hat{y} \\geq 0.5$, we say our prediction is that the team will win; otherwise, we say that we predict that we will lose. A simple way to do this is just to round our $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = predicted_probability_of_winning_given_features(points_and_bias, np.array([beta_hat_1, beta_hat_2])) \n",
    "games_and_predictions = df.copy()\n",
    "games_and_predictions[\"predicted_to_win\"] = np.round(y_hat)\n",
    "games_and_predictions[[\"TEAM_NAME\", \"GAME_DATE\", \"WON\", \"predicted_to_win\"]].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To evaluate the quality of your model, compute the fraction of the rows of the table for which your model was able to correctly predict the outcome of the game based on only the points scored by one team. Assign this to the variable `percentage_correct`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_correct = ...\n",
    "# BEGIN SOLUTION NO PROMPT\n",
    "correct = sum(games_and_predictions['WON'] == games_and_predictions[\"predicted_to_win\"])\n",
    "total = len(games_and_predictions)\n",
    "percentage_correct = correct / total\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q3c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3d\n",
    "\n",
    "Recall that the surface for the MSE has a huge almost completely flat region, which means that the loss function is very difficult to optimize.\n",
    "\n",
    "In lecture on Tuesday 11/5, we'll talk about an alternate loss function called the cross-entropy loss that will yield a much nicer loss surface (no big flat regions). We have defined `cel(y_obs, y_hat)` for you; this function calculates the cross-entropy loss betweeen `y_obs` and `y_hat`. Create a function `cel_for_model_on_NBA_data(beta)` that takes in a value of $\\vec{{\\beta}}$ and returns the cross-entropy loss on the dataset from question 2.\n",
    "\n",
    "**Hint:** Your code for this part should be very similar to your code for question 3a.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3d\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cel(y_obs, y_hat):\n",
    "    return -np.mean(y_obs * np.log(y_hat) + (1 - y_obs) * np.log(1 - y_hat))\n",
    "\n",
    "def cel_for_model_on_NBA_data(beta):\n",
    "    # BEGIN SOLUTION\n",
    "    y_hat = predicted_probability_of_winning_given_features(points_and_bias, beta)\n",
    "    y_obs = df[\"WON\"]\n",
    "    return cel(y_obs, y_hat)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q3d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Cross-Entropy Loss\n",
    "\n",
    "The cell below plots your cross-entropy loss function. Note that the surface has no big flat regions, which makes it easy to optimize.\n",
    "\n",
    "Note: Feel free to ignore the divide by zero warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "num_points = 50 # increase for better resolution, but it will run more slowly. \n",
    "\n",
    "if (num_points <= 100):\n",
    "\n",
    "    uvalues = np.linspace(-0.3, 0.3, num_points)\n",
    "    vvalues = np.linspace(-20, 20, num_points)\n",
    "    (u,v) = np.meshgrid(uvalues, vvalues)\n",
    "    thetas = np.vstack((u.flatten(),v.flatten()))\n",
    "\n",
    "    CEL = np.array([cel_for_model_on_NBA_data(t) for t in thetas.T])\n",
    "\n",
    "    loss_surface = go.Surface(x=u, y=v, z=np.reshape(CEL, u.shape))\n",
    "\n",
    "    fig = go.Figure(data=[loss_surface])\n",
    "    fig.update_layout(scene = dict(\n",
    "        xaxis_title = \"theta0\",\n",
    "        yaxis_title = \"theta1\",\n",
    "        zaxis_title = \"CEL\"))\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Picking num points > 100 can be really slow. If you really want to try, edit the code above so that this if statement doesn't trigger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we only fit a logistic regression model one one feature (the number of points) and an intercept term. However, we can easily train a higher dimensional logistic regression model by using more features that provide useful information. For example, we can add the column `FGM` to our model in addition to `PTS`. Note that this is similar to the difference between simple linear regression and multiple linear regression. As an optional exercise, consider adding more features to your logistic regression model to increase `percentage_correct` from question 3c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure to complete Vitamin 11 on Gradescope by 11:59 PM on Monday, 11/4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Congratulations! You are finished with this assignment. Please don't forget to submit by 11:59pm on Monday, 11/4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to submit.\n",
    "ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
